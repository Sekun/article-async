# Exemple n°5 : Les entrées/sorties, le nerf de la guerre

Vous vous demandez peut-être pourquoi on parle tout le temps d'*IO* en
programmation asynchrone. En effet, les entrées/sorties des programmes
semblent indissociables du concept d'asynchrone, à tel point que cela se
traduit jusque dans le nom de la bibliothèque standard `asyncio` de Python.
Mais *pourquoi* ?

Commençons par une définition : une *IO*, c'est une opération pendant laquelle
un programme *interagit avec un flux de données*. Ce **flux de données** peut
être plein de choses : une connexion réseau, les flux standard `STDIN`,
`STDOUT` ou `STDERR` du processus en cours d'exécution, un fichier, ou même
une abstraction matérielle[^abstraction]. « Interagir avec un flux de
données », ça veut dire l'**ouvrir**, **lire** ou **écrire** dedans ou le
**fermer**.

[^abstraction]: On peut par exemple lire un son sous Linux en *écrivant*
des données dans un fichier spécial qui représente la carte son !

Jusqu'ici, nous avons travaillé sur des exemples très simples qui se
contentaient d'afficher des choses à l'écran pour bien comprendre l'ordre dans
lequel les instructions étaient exécutées. Nos tâches ne réalisaient du coup
que des entrées/sorties, certes, mais celles-ci étaient *synchrones* : on a
considéré jusqu'à maintenant qu'un `print()` dans la console s'exécute
immédiatement et sans délai lors de son appel, ce qui est parfaitement
intuitif...

... mais pas toujours le reflet de la réalité.

Prenons par exemple une IO très simple que vous réalisez en permanence
sur votre ordinateur ou smartphone sans même vous en rendre compte : **que se
passe-t-il entre le moment où vous avez cliqué sur un lien dans une page web,
et celui où le résultat commence à s'afficher sur votre écran** ?

Eh bien vous **attendez**. Tout simplement. Et votre navigateur aussi. Sans
rentrer dans le détail du protocole HTTP, on peut schématiser grossièrement ce
qui se passe comme ceci :


```
        Navigateur                           Serveur web
        ==========                           ===========

           [clic sur le lien]                     .
[création d'une requête HTTP]                     .
       [connexion au serveur]  ------>            .
            .                            [connexion reçue]
            .                  <------   [connexion acceptée]
        [envoi de la requête]  ------>            .
            .                            [réception de la requête]
            .                            [création de la réponse]
            .                  <------   [envoi de la réponse]
    [réception de la réponse]                     .
       [affichage de la page]
```

Dans ce schéma, tous les points (`.`) symbolisent une  attente. Un échange HTTP
(et plus généralement une IO), c'est une opération pendant laquelle les
programmes, passent le plus clair de leur temps à **ne rien faire**. Et votre
navigateur lui-même vous le dit (généralement dans un petit cadre en bas à
gauche de l'écran) :

![](src/img/waiting.png)

Dans ces conditions, l'idée de base de la programmation asynchrone est de
*mettre à profit* tout ce temps que l'on passe à attendre pendant la
réalisation d'une IO pour **s'occuper en faisant autre chose**.

L'exemple du serveur de *fast food* que nous avons modélisé plus tôt n'est pas
anodin ; qu'il s'agisse du serveur *bien réel* d'un restaurant ou celui d'une
application réseau, les deux réalisent en général des opérations comparables.
En effet, de très nombreux serveurs (par exemple d'applications Web)
fonctionnent plus ou moins suivant ce schéma :

1. Recevoir une requête, une commande ou un message,
2. Aller récupérer des ressources à différents endroits,
3. Combiner les ressources entre elles,
4. Répondre au client.

Dans ce schéma, les points 1, 2 et 4 peuvent être des *IO* :

1. Réception :
    * **Attente** d'une connexion,
    * Acceptation de la connexion,
    * **Attente** du message,
    * Réception du message
2. Récupérer des ressources :
    * S'il s'agit de ressources distantes :
        * Connexion à un service,
        * **Attente** de l'acceptation de la connexion,
        * Envoi d'une requête,
        * **Attente** que la réponse arrive,
        * Réception de la réponse,
    * Si la ressource est protégée par un verrou ou un sémaphore :
        * **Attente** de l'acquisition du verrou/sémaphore,
        * Récupération de la ressource,
        * Relâchement du verrou/sémaphore,
3. Combiner les ressources entre elles,
4. Répondre au client :
    * **Attente** que le *medium* soit disponible en écriture,
    * Envoi de la réponse.

Comme vous le voyez, il est vraiment *très* courant d'attendre pour un serveur.
Et il ne s'agit là que d'un schéma particulier dans une infinité d'applications
possibles.

Ainsi, il serait possible d'optimiser de nombreux programmes en les rendant
asynchrones, pour peu que l'on soit capable de rendre leurs *IO* **non
bloquantes**, c'est-à-dire que l'on puisse laisser la main à d'autres tâches au
lieu d'attendre, et reprendre celle-ci avec l'assurance que l'on pourra
réaliser une *IO* immédiatement.

Il existe sous la plupart des systèmes d'exploitation une fonctionnalité qui
permet de déterminer à un instant donné si un ou plusieurs flux de données sont
accessibles en lecture ou en écriture. En fait, il en existe plein, mais nous
allons nous concentrer sur celle qui sera disponible sur la plupart des
systèmes d'exploitation : il s'agit de l'appel-système
[`select()`](https://docs.python.org/3.4/library/select.html#select.select).

Celui-ci, en Python, se présente sous la forme suivante :

```python
select.select(rlist, wlist, xlist[, timeout])
```

Où :

* `rlist` est une liste de flux sur lesquels nous voulons lire des données,
* `wlist` est une liste de flux dans lesquels nous voulons écrire des données,
* `xlist` est une liste de flux que l'on surveille jusqu'à ce qu'il se produise
  des *conditions exceptionnelles* (ne nous attardons pas là-dessus),
* `timeout` est une durée (optionnelle) pendant laquelle on attend que des flux
  soient disponibles. Par défaut, on attend indéfiniment. Si on lui passe la
  valeur `0`, l'appel à `select()` retourne immédiatement.

Cette fonction retourne trois listes :

* une contenant les flux disponibles en lecture à l'instant T,
* une contenant les flux disponibles en écriture à l'instant T,
* une autre contenant les flux victimes d'une exception.

Que demander de plus ? Nous avons à notre disposition une fonction, dont
l'exécution est instantannée, qui pourra nous permettre de réveiller les tâches
en attente de lecture ou d'écriture.

On peut donc implémenter les fonctions d'attente asynchrones suivantes :

```python
from select import select

# Attendre de façon asynchrone qu'un flux soit disponible en lecture
def wait_readable(stream):
    while True:
        rlist, _, _ = select([stream], [], [], 0)
        if rlist:
            return stream
        yield

# Attendre de façon asynchrone qu'un flux soit disponible en écriture
def wait_writable(stream):
    while True:
        _, wlist, _ = select([], [stream], [], 0)
        if wlist:
            return stream
        yield
```

**Note:** Sous la plupart des systèmes d'exploitation Unix, vous pouvez
utiliser ces fonction pour attendre après n'importe quel flux de données (flux
standard, fichiers, sockets), mais sous Windows, *seules* les sockets sont
supportées.

Pour bien comprendre l'apport de ces deux fonctions, commençons par écrire un
petit serveur qui se contente d'attendre une seconde avant de renvoyer les
messages des clients :

```python
from socket import socket
from time import sleep

def echo_server():
    sock = socket()  # Création d'une socket TCP
    sock.bind(('localhost', 1234)) # On écoute sur le port 12345
    sock.listen(5)  # Accepter jusqu'à 5 connexions simultanées
    try:
        while True:
            conn, host = sock.accept()
            msg = conn.recv(4096)
            print("message reçu de {!r}: {}".format(host, msg.decode()))
            conn.send(msg)
            conn.close()
    finally:
        sock.close()
```

Lançons ce serveur dans une console.

```python
>>> echo_server()
```

Dans **une autre console**, nous pouvons vérifier que celui-ci fonctionne en
lui envoyant un message :

```python
>>> sock = socket.socket()
>>> sock.connect(('localhost', 1234))
>>> sock.send("Ohé !".encode())
6
```

Le serveur affiche alors :

    message reçu de ('127.0.0.1', 40568): Ohé !

Nous n'avons plus qu'à récupérer sa réponse dans la fenêtre du client :

```python
>>> data = sock.recv(1024)
>>> data.decode()
'Ohé !'
```

Parfait.

Pour corser les choses, essayons maintenant de lui envoyer 5 messages à la
fois. Utilisons pour cela `asyncio`, ainsi que les deux coroutines d'attente
que nous venons d'écrire :

```python
from socket import socket
from datetime import datetime

@asyncio.coroutine
def echo(msg):
    sock = socket()
    sock.connect(('localhost', 1234))
    yield from wait_writable(sock)
    print("Envoi du message {!r}".format(msg))
    sock.send(msg.encode())
    yield from wait_readable(sock)
    data = sock.recv(4096)
    print("Message reçu:", data.decode())
    sock.close()

@asyncio.coroutine
def echo_client():
    start = datetime.now()
    tasks = [echo("Hello {}".format(num)) for num in range(5)]
    yield from asyncio.wait(tasks)
    print("temps total:", datetime.now() - start)
```

Rien de fondamentalement compliqué : la coroutine `echo()` se contente de faire
ce que nous avons réalisé juste avant dans la console, mais en attendant de
façon asynchrone que la socket soit disponible en écriture, puis en lecture.

Voici le résultat :

```python
>>> loop = asyncio.get_event_loop()
>>> loop.run_until_complete(echo_client())
Envoi du message 'Hello 2'
Envoi du message 'Hello 1'
Envoi du message 'Hello 0'
Envoi du message 'Hello 3'
Envoi du message 'Hello 4'
Message reçu: Hello 2
Message reçu: Hello 1
Message reçu: Hello 0
Message reçu: Hello 3
Message reçu: Hello 4
temps total: 0:00:05.013461
```

À l'exécution, nous nous rendons compte que le serveur, qui est *synchrone*,
traite les messages les uns à la suite des autres. Réimplémentons-le avec
`asyncio` :

```python
@asyncio.coroutine
def serve_echo(conn, host):
    yield from wait_readable(conn)
    msg = conn.recv(4096)
    print("message reçu de {!r}: {}".format(host, msg.decode()))
    yield from asyncio.sleep(1)
    yield from wait_writable(conn)
    conn.send(msg)
    conn.close()

@asyncio.coroutine
def echo_server():
    sock = socket()  # Création d'une socket TCP
    sock.bind(('localhost', 1234)) # On écoute sur le port 12345
    sock.listen(5)  # Accepter jusqu'à 5 connexions simultanées
    try:
        while True:
            yield from wait_readable(sock)
            conn, host = sock.accept()
            asyncio.async(serve_echo(conn, host))
    finally:
        sock.close()
```

Lançons le serveur :

```python
>>> loop = asyncio.get_event_loop()
>>> loop.run_until_complete(echo_server())
```

Puis le client :

```python
>>> loop.run_until_complete(echo_client())
Envoi du message 'Hello 3'
Envoi du message 'Hello 0'
Envoi du message 'Hello 2'
Envoi du message 'Hello 1'
Envoi du message 'Hello 4'
Message reçu: Hello 3
Message reçu: Hello 0
Message reçu: Hello 2
Message reçu: Hello 1
Message reçu: Hello 4
temps total: 0:00:01.001991
```

Cette fois-ci, le serveur a traité toutes les requêtes en même temps, pour un
temps d'exécution total de 1s au lieu de 5s.

Nous voici maintenant capables de réaliser des *IO* asynchrones ! Néanmoins, bien
que nos coroutines `wait_readable()` et `wait_writable()` soient fonctionnent
tout à fait dans cet exemple, je vous **déconseille très vivement** de les
utiliser. En effet, celles-ci bouclent à l'infini jusqu'à ce que le flux soit
disponible, au lieu d'attendre passivement sans consommer de temps sur le
processeur.

Mais rassurez-vous, `asyncio` propose de nombreuses façons de réaliser la même
chose, et ce sans surcharger le processeur. Nous les examinerons dans les
exemples suivants.
